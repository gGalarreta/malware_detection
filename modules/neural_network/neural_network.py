import numpy as np
import tensorflow as tf
import keras
import os
from keras import backend as K
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Dropout, Flatten, BatchNormalization, Activation,\
                        Input, LeakyReLU
from keras.layers.merge import concatenate
from keras.optimizers import Adam, SGD
from keras.callbacks import ModelCheckpoint
from keras.models import Model
from keras.utils import np_utils



class ConvNet():

  def __init__(self):
    self.input_size = (1, 128)
    self.pool_size = 2
    self.kernel_size = 2

    self.conv_depth_1 = 32
    self.conv_depth_2 = 64
    self.conv_depth_3 = 128
    self.conv_depth_4 = 256
    self.alpha = 0.1


    self.classes_number = 8
    self.hidden_size = 512

    self.batch_size = 100
    self.epochs = 10

    self.classes = []



  def set_pool_size(self, pool_size):
    self.pool_size = pool_size

  def set_kernel_size(self, kernel_size):
    self.kernel_size = kernel_size

  def set_batch_size(self, batch_size):
    self.batch_size = batch_size

  def set_hidden_size(self, hidden_size):
    self.hidden_size = hidden_size

  def set_epochs(self, epochs):
    self.epochs = epochs

  def set_depths(self, value1, value2, value3, value4):
    self.conv_depth_1 = value1
    self.conv_depth_2 = value2
    self.conv_depth_3 = value3
    self.conv_depth_4 = value4


  def set_classes_number(self, classes_number):
    self.classes_number = classes_number

  def network_model(self, data_directory, classes_directory):

    X_train, Y_train = self.set_data(data_directory, classes_directory)
    #Y_train = np_utils.to_categorical(Y_train_without_one_hot, self.classes_number) 
    
    inputs = Input(self.input_size)

    #input1

    #conv layer 1
    input1_conv1 = Conv1D(self.conv_depth_1, self.kernel_size, padding='same', kernel_initializer='he_uniform')(inputs)
    input1_conv1 = BatchNormalization()(input1_conv1)
    input1_conv1 = LeakyReLU(alpha=self.alpha)(input1_conv1)

    #conv layer 2
    input1_conv2 = Conv1D(self.conv_depth_1, self.kernel_size, padding='same', kernel_initializer='he_uniform')(input1_conv1)
    input1_conv2 = BatchNormalization()(input1_conv2)
    input1_conv2 = LeakyReLU(alpha=self.alpha)(input1_conv2)

    #conv layer 3
    input1_conv3 = Conv1D(self.conv_depth_2, self.kernel_size, padding='same', kernel_initializer='he_uniform')(input1_conv2)
    input1_conv3 = BatchNormalization()(input1_conv3)
    input1_conv3 = LeakyReLU(alpha=self.alpha)(input1_conv3)

    #input 2

    #conv layer 1
    input2_conv1 = Conv1D(self.conv_depth_1, self.kernel_size, padding='same', kernel_initializer='he_uniform')(inputs)
    input2_conv1 = BatchNormalization()(input2_conv1)
    input2_conv1 = LeakyReLU(alpha=self.alpha)(input2_conv1)

    #conv layer 2
    input2_conv2 = Conv1D(self.conv_depth_1, self.kernel_size, padding='same', kernel_initializer='he_uniform')(input2_conv1)
    input2_conv2 = BatchNormalization()(input2_conv2)
    input2_conv2 = LeakyReLU(alpha=self.alpha)(input2_conv2)

    #conv layer 3
    input2_conv3 = Conv1D(self.conv_depth_2, self.kernel_size, padding='same', kernel_initializer='he_uniform')(input2_conv2)
    input2_conv3 = BatchNormalization()(input2_conv3)
    input2_conv3 = LeakyReLU(alpha=self.alpha)(input2_conv3)

    x = concatenate([input1_conv3, input2_conv3])

    #conv layer 1
    x_conv1 = Conv1D(self.conv_depth_3, self.kernel_size, padding='same', kernel_initializer='he_uniform')(x)
    x_conv1 = BatchNormalization()(x_conv1)
    x_conv1 = LeakyReLU(alpha=self.alpha)(x_conv1)


    #conv layer 1
    x_conv2 = Conv1D(self.conv_depth_3, self.kernel_size, padding='same', kernel_initializer='he_uniform')(x_conv1)
    x_conv2 = BatchNormalization()(x_conv2)
    x_conv2 = LeakyReLU(alpha=self.alpha)(x_conv2)

    #conv layer 1
    x_conv3 = Conv1D(self.conv_depth_4, self.kernel_size, padding='same', kernel_initializer='he_uniform')(x_conv2)
    x_conv3 = BatchNormalization()(x_conv3)
    x_conv3 = LeakyReLU(alpha=self.alpha)(x_conv3)


    x = Flatten()(x_conv3)
    x = Dense(self.hidden_size, activation='relu')(x)
    prediction = Dense(self.classes_number, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=prediction)
    model.summary()

    model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

    hist_recog = model.fit(np.reshape(X_train, (X_train.shape[0],1,X_train.shape[1])),Y_train,batch_size=self.batch_size, epochs=self.epochs, verbose=1)


  def set_data(self, data_directory, classes_directory):
    self.set_settings(classes_directory)
    file = open(data_directory, 'r')
    features_list = []
    classes_list = []
    features_number = 0
    samples_number = 0
    for line in file:
      sample_class, features = self.get_values(line.split(","))
      features_list =  features + features_list
      classes_list = [sample_class] + classes_list
      samples_number += 1
      features_number = len(features)
    self.set_input_size(1, features_number)
    return np.asarray(features_list).reshape(samples_number, features_number), np.asarray(classes_list).reshape(samples_number, 1)

  def set_settings(self, classes_directory):
    self.classes = self.read_file(classes_directory)
    self.classes_number = len(self.classes)

  def get_values(self, data):
    #0 -> name
    #1 .. n-1 -> features
    #n -> class
    #number of features must be the same with the quantity of elements of the list resulting TODO
    return self.get_class(data), self.get_features(data)

  def get_class(self, data):
    sample_class = data[-2]
    return self.classes.index(sample_class)

  def get_features(self, data):
    #remove class value and \n
    data.pop()
    data.pop()
    #remove sample_name
    data.pop(0)
    return data

  def read_file(self, file_name):
    lines = [line.rstrip('\n') for line in open(file_name)]
    #little bug :(  TODO
    #temp fix
    lines = [word.lower() for word in lines]
    return lines

  def set_input_size(self, lower_size, upper_size):
    self.input_size = (lower_size, upper_size)