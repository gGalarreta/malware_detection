#from keras.preprocessing import sequence
#from keras.models import Sequential
#from keras.layers import Dense, Embedding
#from keras.layers import LSTM
from scipy.sparse import *
import numpy as np
import os


class RnnLstm():

  def __init__(self):
    self.x_train = None
    self.y_train = None
    self.x_test = None
    self.y_test = None
    self.max_features = 8000
    self.max_len = 20
    #self.model = Sequential()
    self.output_dim = 128
    self.dropout = 0.2
    self.recurrent_dropout = 0.2
    self.number_classes = 2
    self.activation = 'softmax'
    self.batch_size = 32
    self.epochs = 2
    self.score = None
    self.acc = None
    self.train_data_set_directory = None
    self.test_data_set_directory = None
    self.classes_directory = None

  def set_max_features(self, value):
    self.max_features = value

  def set_max_len(self, value):
    self.max_len = value

  def set_output_dim(self, value):
    self.output_dim = value

  def set_number_classes(self, value):
    self.number_classes = value

  def set_batch_size(self, value):
    self.batch_size = value

  def set_epochs(self, value):
    self.epochs = value

  def set_train_data_set_directory(self, path):
    self.train_data_set_directory = path

  def set_test_data_set_directory(self, path):
    self.test_data_set_directory = path

  def set_classes_directory(self, path):
    self.classes_directory = path

  def build(self):
    #self.x_train = sequence.pad_sequences(self.x_train, maxlen = self.max_len)
    #self.x_test = sequence.pad_sequences(self.x_test, maxlen = self.max_len)

    self.model.add(Embedding(self.max_features, self.output_dim))
    self.model.add(LSTM(self.output_dim, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout))

    self.model.add(Dense(self.number_classes, activation = self.activation))
    self.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    self.model.summary()

  def train():
    self.model.fit(self.x_train, self.y_train, batch_size = self.batch_size, epochs = self.epochs, validation_data = (self.x_test, self.y_test))

  def accurate():
    self.score, self.acc = self.model.evaluate(self.x_test, self.y_test, batch_size = self.batch_size)

  def get_data(self):
    opcodes_to_indices, indices_to_opcodes, unique_opcodes = self.get_direccionary();
    #max_len = 20
    step = 50
    sentences = []
    #next_chars = []
    directory = self.train_data_set_directory
    for folder in os.listdir(directory):
      index = self.read_file(directory + "/" + folder + "/" + "index.txt")
      for file in index:
        file_path = directory + "/" + folder + "/" + "opcode/" + file + ".txt"
        opcodes = self.read_file(file_path)
        for i in range(0, len(opcodes) - self.max_len, step):
            sentences.append(opcodes[i: i + self.max_len])
            #asignar clases


    unique_data = [list(x) for x in set(tuple(x) for x in sentences)]
    print len(unique_data)

    #values = [42]
    #row_ind = [211147]
    #col_ind = [9] 
    #last_array = csc_matrix((values, (row_ind, col_ind)), shape=(211148,211148))
    #print last_array

    #print(last_array[211147,9])
    #x = np.zeros((len(sentences), self.max_len, len(unique_data)))
    #for i, sentence in enumerate(sentences):
     #   for t, char in enumerate(sentence):
            # mark the each corresponding character in a sentence as 1
      #      x[i, t, char_to_indices[char]] = 1
    #print x


    # 1-of-k encoding (all zeros except for a single one at
    # the index of the character in the vocab)
    # all input sentences encoded
    # expected outputs for each sentence
    #y = np.zeros((len(sentences), len(chars)))
        # mark the corresponding character in expected output as 1
        #y[i, char_to_indices[next_chars[i]]] = 1

    #return text, max_len, len(chars), char_to_indices, indices_to_chars, x, y    

  def get_direccionary(self):
    train_unique_opcodes = self.load_files(self.train_data_set_directory)
    test_unique_opcodes = self.load_files(self.test_data_set_directory)
    unique_opcodes = list(set(train_unique_opcodes + test_unique_opcodes))
    opcodes_to_indices = dict((char, idx) for idx, char in enumerate(unique_opcodes))
    indices_to_opcodes = dict((idx, char) for idx, char in enumerate(unique_opcodes))
    return opcodes_to_indices, indices_to_opcodes, unique_opcodes

  def load_files(self, directory):
    opcodes_vocabulary = []
    for folder in os.listdir(directory):
      index = self.read_file(directory + "/" + folder + "/" + "index.txt")
      for file in index:
        file_path = directory + "/" + folder + "/" + "opcode/" + file + ".txt"
        opcodes = self.read_file(file_path)
        opcodes_vocabulary = opcodes_vocabulary + list(set(opcodes))
    return opcodes_vocabulary

  def read_file(self, file_name):
    lines = [line.rstrip('\n') for line in open(file_name)]
    return lines

