from sklearn.feature_extraction.text import TfidfVectorizer
from raw_analysis import raw_analysis
from features_handler.k_means import k_means
from features_handler.agglomerative import agglomerative_cluster
import sys
import pefile
import pydasm
import os
import numpy as np

class Opcode():

  def __init__(self):
    self.n_grams = 5

  def disassemble(self, pe_data, file_name):
    file = open(file_name, "w")
    try:
      ep = pe_data.OPTIONAL_HEADER.AddressOfEntryPoint
      ep_ava = ep+pe_data.OPTIONAL_HEADER.ImageBase
      data = pe_data.get_memory_mapped_image()[ep:ep + self.size_of_raw_data(pe_data)]
      offset = 0
      while offset < len(data):
        i = pydasm.get_instruction(data[offset:], pydasm.MODE_32)
        instruction = pydasm.get_instruction_string(i, pydasm.FORMAT_INTEL, ep_ava+offset)
        if instruction == None :
          break
        offset += i.length
        file.write(instruction.split(' ', 1)[0])
        file.write('\n')
    except Exception as e:
      print "This sample has some ofuscated values"

  def size_of_raw_data(self, pe_data):
    raw_analyzer = raw_analysis.Raw()
    return raw_analyzer.size_of_raw_data(pe_data)


  def get_dendograms(self, input_directory, rule, folder, index):
    agglomerative_classifier = self.agglomerative_settings(rule)
    data_list = self.get_opcodes_from_data(input_directory)
    vectorizer = TfidfVectorizer(min_df=0.05)
    X = vectorizer.fit_transform(data_list)
    idf = vectorizer.idf_
    data = np.reshape(idf, (len(idf), 1))
    agglomerative_classifier.set_cut_off(len(idf))
    output_file = os.getcwd() + "/features_handler/agglomerative/dendograms/" + folder + "_opcodes.png"
    agglomerative_classifier.get_dendograms(data, output_file)


  def agglomerative_settings(self, rule):
    agglomerative_classifier = agglomerative_cluster.Agglomerative()
    return agglomerative_classifier


  def extract_features(self, input_directory, output_directory, rule):
    cluster = self.cluster_settings(rule)
    data_list = self.get_opcodes_from_data(input_directory)
    self.get_top_features(cluster, data_list, output_directory)

  def cluster_settings(self, rule):
    cluster_classifier = k_means.Kmeans()
    vectorizer =  TfidfVectorizer(max_df = float(rule["max_df_tfidvectorize"]), max_features = int(rule["max_features_tfidfvectorize"]), min_df = 0.05, use_idf=True, decode_error='ignore', ngram_range=(int(rule["max_n_grams_opcodes"]), self.n_grams))
    cluster_classifier.set_number_top_words(int(rule["number_of_extracted_top_features"]))
    cluster_classifier.set_number_of_clusters(int(rule["opcodes_number_of_clusters"]))
    cluster_classifier.set_vectorizer(vectorizer)
    return cluster_classifier    

  def get_opcodes_from_data(self, input_directory):
    files = os.listdir(input_directory)
    opcodes_lists = []
    for file in files:
      file_name = input_directory + "/" + file
      file_opcodes = self.list_string(self.read_file(file_name))
      opcodes_lists.append(file_opcodes)
    return opcodes_lists

  def read_file(self,file_name):
    lines = [line.rstrip('\n') for line in open(file_name)]
    return lines

  def list_string(self, data_list):
    document = ','.join(data_list)
    return document

  def get_top_features(self, cluster, data_list, output_directory):
    cluster.set_data(data_list)
    cluster.extract_top_features(output_directory)

  def ngrams(self, data_list):
    n_gram_vector = self.ngrams_structure(data_list, self.n_grams)
    return self.unique_list_of_lists(n_gram_vector)

  def ngrams_structure(self, data_list, size = 2, i = 0):
    while len(data_list[i:i+size]) == size:
      yield data_list[i:i+size]
      i +=1

  def unique_list_of_lists(self, data_list):
    unique_data = []
    for ngram_tuple in set(tuple(ngram) for ngram in data_list):
      unique_data = unique_data + [' '.join(list(ngram_tuple))]
    return unique_data
